{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "BOS_TOKEN = \"시\"\n",
    "EOS_TOKEN = \"끝\"\n",
    "SPLIT_TOKEN = \"▁\"\n",
    "def create_digest_cedict(mono_file, poly_file, output_file):\n",
    "    cedict = {}\n",
    "\n",
    "    # Process monophonic characters\n",
    "    with open(mono_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            char, pron = line.strip().split('\\t')\n",
    "            cedict[char] = [pron]\n",
    "\n",
    "    # Process polyphonic characters\n",
    "    with open(poly_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            char, prons = line.strip().split('\\t')\n",
    "            cedict[char] = prons.split(',')\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cedict, f)\n",
    "\n",
    "# Create digest_cedict.pkl\n",
    "create_digest_cedict('MONOPHONIC_CHARS.txt', 'POLYPHONIC_CHARS.txt', 'digest_cedict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def create_char2idx(sent_files, output_file):\n",
    "    char2idx = {}\n",
    "    idx = 0\n",
    "\n",
    "    for sent_file in sent_files:\n",
    "        with open(sent_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for char in line.strip():\n",
    "                    if char not in char2idx:\n",
    "                        char2idx[char] = idx\n",
    "                        idx += 1\n",
    "\n",
    "    # Add special tokens\n",
    "    char2idx[UNK_TOKEN] = idx\n",
    "    char2idx[PAD_TOKEN] = idx + 1\n",
    "    char2idx[BOS_TOKEN] = idx + 2\n",
    "    char2idx[EOS_TOKEN] = idx + 3\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(char2idx, f)\n",
    "\n",
    "\n",
    "# Create char2idx.pkl\n",
    "create_char2idx([\"train.sent\", \"dev.sent\", \"test.sent\"], \"char2idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class2idx(lb_files, output_file):\n",
    "    class2idx = {}\n",
    "    idx = 0\n",
    "\n",
    "    for lb_file in lb_files:\n",
    "        with open(lb_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for pron in line.strip().split():\n",
    "                    if pron not in class2idx:\n",
    "                        class2idx[pron] = idx\n",
    "                        idx += 1\n",
    "\n",
    "    # Add special tokens\n",
    "    class2idx[UNK_TOKEN] = idx\n",
    "    class2idx[PAD_TOKEN] = idx + 1\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(class2idx, f)\n",
    "\n",
    "\n",
    "# Create class2idx.pkl\n",
    "create_class2idx([\"train.lb\", \"dev.lb\", \"test.lb\"], \"class2idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "def initialize_np_ckpt(char2idx, class2idx, embedding_dim=64, lstm_hidden_dim=32):\n",
    "    state_dict = {}\n",
    "\n",
    "    # Initialize embedding weights\n",
    "    state_dict[\"embedding.weight\"] = np.random.randn(\n",
    "        len(char2idx), embedding_dim\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Initialize LSTM weights and biases\n",
    "    state_dict[\"lstm.weight_ih_l0\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, embedding_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.weight_hh_l0\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.bias_ih_l0\"] = np.zeros(4 * lstm_hidden_dim, dtype=np.float32)\n",
    "    state_dict[\"lstm.bias_hh_l0\"] = np.zeros(4 * lstm_hidden_dim, dtype=np.float32)\n",
    "\n",
    "    state_dict[\"lstm.weight_ih_l0_reverse\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, embedding_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.weight_hh_l0_reverse\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.bias_ih_l0_reverse\"] = np.zeros(\n",
    "        4 * lstm_hidden_dim, dtype=np.float32\n",
    "    )\n",
    "    state_dict[\"lstm.bias_hh_l0_reverse\"] = np.zeros(\n",
    "        4 * lstm_hidden_dim, dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Initialize fully connected layer weights and biases\n",
    "    state_dict[\"logit_layer.0.weight\"] = np.random.randn(\n",
    "        lstm_hidden_dim, 2 * lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"logit_layer.0.bias\"] = np.zeros(lstm_hidden_dim, dtype=np.float32)\n",
    "    state_dict[\"logit_layer.2.weight\"] = np.random.randn(\n",
    "        len(class2idx), lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"logit_layer.2.bias\"] = np.zeros(len(class2idx), dtype=np.float32)\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(\"np_ckpt.pkl\", \"wb\") as f:\n",
    "        pickle.dump(state_dict, f)\n",
    "\n",
    "\n",
    "# Load char2idx and class2idx\n",
    "char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "\n",
    "# Initialize np_ckpt.pkl\n",
    "initialize_np_ckpt(char2idx, class2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_cedict = pickle.load(open(\"digest_cedict.pkl\", \"rb\"))\n",
    "char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "\n",
    "# Print statistics\n",
    "print(\"Length of digest_cedict:\", len(digest_cedict))\n",
    "print(\"Length of char2idx:\", len(char2idx))\n",
    "print(\"Length of class2idx:\", len(class2idx))\n",
    "state_dict = pickle.load(open(\"np_ckpt.pkl\", \"rb\"))\n",
    "\n",
    "print(\"Dimensions of embedding.weight:\", state_dict[\"embedding.weight\"].shape)\n",
    "print(\"Dimensions of lstm.weight_ih_l0:\", state_dict[\"lstm.weight_ih_l0\"].shape)\n",
    "print(\"Dimensions of lstm.weight_hh_l0:\", state_dict[\"lstm.weight_hh_l0\"].shape)\n",
    "print(\"Dimensions of lstm.bias_ih_l0:\", state_dict[\"lstm.bias_ih_l0\"].shape)\n",
    "print(\"Dimensions of lstm.bias_hh_l0:\", state_dict[\"lstm.bias_hh_l0\"].shape)\n",
    "print(\n",
    "    \"Dimensions of lstm.weight_ih_l0_reverse:\",\n",
    "    state_dict[\"lstm.weight_ih_l0_reverse\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"Dimensions of lstm.weight_hh_l0_reverse:\",\n",
    "    state_dict[\"lstm.weight_hh_l0_reverse\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"Dimensions of lstm.bias_ih_l0_reverse:\",\n",
    "    state_dict[\"lstm.bias_ih_l0_reverse\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"Dimensions of lstm.bias_hh_l0_reverse:\",\n",
    "    state_dict[\"lstm.bias_hh_l0_reverse\"].shape,\n",
    ")\n",
    "print(\"Dimensions of logit_layer.0.weight:\", state_dict[\"logit_layer.0.weight\"].shape)\n",
    "print(\"Dimensions of logit_layer.0.bias:\", state_dict[\"logit_layer.0.bias\"].shape)\n",
    "print(\"Dimensions of logit_layer.2.weight:\", state_dict[\"logit_layer.2.weight\"].shape)\n",
    "print(\"Dimensions of logit_layer.2.bias:\", state_dict[\"logit_layer.2.bias\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from g2pM2 import G2pM\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(sent_file, lb_file):\n",
    "    with open(sent_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = [line.strip() for line in f]\n",
    "    with open(lb_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        labels = [line.strip().split() for line in f]\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "# Convert characters and labels to indices and pad sequences\n",
    "def prepare_data(sentences, labels, char2idx, class2idx):\n",
    "    input_ids = []\n",
    "    target_ids = []\n",
    "    target_indices = []\n",
    "    for sent, label in zip(sentences, labels):\n",
    "        input_id = [char2idx.get(char, char2idx[UNK_TOKEN]) for char in sent]\n",
    "        target_id = [class2idx.get(pron, class2idx[UNK_TOKEN]) for pron in label]\n",
    "        input_ids.append(input_id)\n",
    "        target_ids.append(target_id)\n",
    "\n",
    "        # Compute target indices for polyphonic characters\n",
    "        target_idx = [i for i, pron in enumerate(label) if pron in class2idx]\n",
    "        target_indices.append(target_idx)\n",
    "\n",
    "    # Pad sequences\n",
    "    max_length = max(len(seq) for seq in input_ids)\n",
    "    input_ids = [\n",
    "        seq + [char2idx[PAD_TOKEN]] * (max_length - len(seq)) for seq in input_ids\n",
    "    ]\n",
    "    target_ids = [\n",
    "        seq + [class2idx[PAD_TOKEN]] * (max_length - len(seq)) for seq in target_ids\n",
    "    ]\n",
    "\n",
    "    return np.array(input_ids), np.array(target_ids), target_indices\n",
    "\n",
    "\n",
    "# Generate batches of data\n",
    "def get_batches(data, batch_size):\n",
    "    inputs, targets, target_indices = data\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch_inputs = inputs[i : i + batch_size]\n",
    "        batch_targets = targets[i : i + batch_size]\n",
    "        batch_target_indices = target_indices[i : i + batch_size]\n",
    "        yield np.array(batch_inputs), np.array(batch_targets), batch_target_indices\n",
    "\n",
    "\n",
    "def compute_loss(model, inputs, targets, target_indices):\n",
    "    lengths = np.sum(np.sign(inputs), axis=1)\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    # Recompute the hidden states up to the point where logits are obtained\n",
    "    rev_seq = model.reverse_sequence(inputs, lengths)\n",
    "    fw_emb = model.get_embedding(inputs)  # [b, t, d]\n",
    "    bw_emb = model.get_embedding(rev_seq)\n",
    "\n",
    "    fw_states, bw_states = None, None\n",
    "    fw_hs = []\n",
    "    bw_hs = []\n",
    "    for i in range(max_length):\n",
    "        fw_input = fw_emb[:, i, :]\n",
    "        bw_input = bw_emb[:, i, :]\n",
    "        fw_states = model.fw_lstm_cell(fw_input, fw_states)\n",
    "        bw_states = model.bw_lstm_cell(bw_input, bw_states)\n",
    "\n",
    "        fw_hs.append(fw_states[0])\n",
    "        bw_hs.append(bw_states[0])\n",
    "    fw_hiddens = np.stack(fw_hs, axis=1)\n",
    "    bw_hiddens = np.stack(bw_hs, axis=1)\n",
    "    bw_hiddens = model.reverse_sequence(bw_hiddens, lengths)\n",
    "\n",
    "    outputs = np.concatenate([fw_hiddens, bw_hiddens], axis=2)  # [b, t, d]\n",
    "    batch_size = outputs.shape[0]\n",
    "    if batch_size == 1:\n",
    "        outputs = outputs.squeeze(axis=0)  # [t, d]\n",
    "        target_hidden = outputs[target_indices[0]]\n",
    "    else:\n",
    "        # Flatten target_indices for batch processing\n",
    "        target_hidden = []\n",
    "        for i in range(batch_size):\n",
    "            for idx in target_indices[i]:\n",
    "                target_hidden.append(outputs[i, idx])\n",
    "        target_hidden = np.array(target_hidden)  # [total_targets, d]\n",
    "\n",
    "    # Compute logits using the fc_layer\n",
    "    logits = model.fc_layer(target_hidden)  # [total_targets, num_classes]\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    softmax_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    # Extract the probabilities of the correct classes\n",
    "    total_targets = len(target_hidden)\n",
    "    target_classes = []\n",
    "    for i in range(batch_size):\n",
    "        for idx in target_indices[i]:\n",
    "            target_classes.append(targets[i, idx])\n",
    "    target_classes = np.array(target_classes)  # [total_targets]\n",
    "\n",
    "    # Gather the probabilities for the target classes\n",
    "    target_probs = softmax_probs[np.arange(total_targets), target_classes]\n",
    "\n",
    "    # Compute the negative log likelihood\n",
    "    loss = -np.log(target_probs + 1e-9)\n",
    "    loss = np.sum(loss) / total_targets\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Update the model weights using Adam optimizer\n",
    "def update_weights(model, grads, learning_rate, beta1, beta2, epsilon, t, m, v):\n",
    "    for param, grad in grads.items():\n",
    "        m[param] = beta1 * m[param] + (1 - beta1) * grad\n",
    "        v[param] = beta2 * v[param] + (1 - beta2) * (grad**2)\n",
    "        m_hat = m[param] / (1 - beta1**t)\n",
    "        v_hat = v[param] / (1 - beta2**t)\n",
    "        model.__dict__[param] -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "def save_model(model, output_file):\n",
    "    state_dict = {\n",
    "        \"embedding.weight\": model.embeddings,\n",
    "        \"lstm.weight_ih_l0\": model.weight_ih,\n",
    "        \"lstm.weight_hh_l0\": model.weight_hh,\n",
    "        \"lstm.bias_ih_l0\": model.bias_ih,\n",
    "        \"lstm.bias_hh_l0\": model.bias_hh,\n",
    "        \"lstm.weight_ih_l0_reverse\": model.weight_ih_reverse,\n",
    "        \"lstm.weight_hh_l0_reverse\": model.weight_hh_reverse,\n",
    "        \"lstm.bias_ih_l0_reverse\": model.bias_ih_reverse,\n",
    "        \"lstm.bias_hh_l0_reverse\": model.bias_hh_reverse,\n",
    "        \"logit_layer.0.weight\": model.hidden_weight_l0,\n",
    "        \"logit_layer.0.bias\": model.hidden_bias_l0,\n",
    "        \"logit_layer.2.weight\": model.hidden_weight_l1,\n",
    "        \"logit_layer.2.bias\": model.hidden_bias_l1,\n",
    "    }\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(state_dict, f)\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    UNK_TOKEN = \"<UNK>\"\n",
    "    PAD_TOKEN = \"<PAD>\"\n",
    "    BOS_TOKEN = \"시\"\n",
    "    EOS_TOKEN = \"끝\"\n",
    "    SPLIT_TOKEN = \"▁\"\n",
    "\n",
    "    # Load the model\n",
    "    model = G2pM()\n",
    "\n",
    "    # Load the training and development data\n",
    "    train_sentences, train_labels = load_data(\"train.sent\", \"train.lb\")\n",
    "    dev_sentences, dev_labels = load_data(\"dev.sent\", \"dev.lb\")\n",
    "\n",
    "    # Load the char2idx and class2idx mappings\n",
    "    char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "    class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "\n",
    "    # Prepare the data\n",
    "    train_data = prepare_data(train_sentences, train_labels, char2idx, class2idx)\n",
    "    dev_data = prepare_data(dev_sentences, dev_labels, char2idx, class2idx)\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Initialize Adam optimizer parameters\n",
    "    t = 0\n",
    "    m = {\n",
    "        param: np.zeros_like(value)\n",
    "        for param, value in model.__dict__.items()\n",
    "        if isinstance(value, np.ndarray)\n",
    "    }\n",
    "    v = {\n",
    "        param: np.zeros_like(value)\n",
    "        for param, value in model.__dict__.items()\n",
    "        if isinstance(value, np.ndarray)\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        total_targets = 0\n",
    "        with tqdm(total=len(train_data[0]), desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "            for inputs, targets, target_indices in get_batches(train_data, batch_size):\n",
    "                t += 1\n",
    "                loss = compute_loss(model, inputs, targets, target_indices)\n",
    "                train_loss += loss\n",
    "\n",
    "                # Compute gradients (this is a placeholder, you need to implement backpropagation to get actual gradients)\n",
    "                grads = {\n",
    "                    param: np.zeros_like(value)\n",
    "                    for param, value in model.__dict__.items()\n",
    "                    if isinstance(value, np.ndarray)\n",
    "                }\n",
    "\n",
    "                # Update weights\n",
    "                update_weights(\n",
    "                    model, grads, learning_rate, beta1, beta2, epsilon, t, m, v\n",
    "                )\n",
    "\n",
    "                # Update progress bar\n",
    "                target_count = sum(len(indices) for indices in target_indices)\n",
    "                total_targets += target_count\n",
    "                pbar.update(len(inputs))\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"Train Loss\": (\n",
    "                            train_loss / total_targets if total_targets > 0 else 0.0\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Validation loop\n",
    "        dev_loss = 0\n",
    "        total_dev_targets = 0\n",
    "        for inputs, targets, target_indices in get_batches(dev_data, batch_size):\n",
    "            loss = compute_loss(model, inputs, targets, target_indices)\n",
    "            dev_loss += loss\n",
    "            total_dev_targets += sum(len(indices) for indices in target_indices)\n",
    "\n",
    "        avg_train_loss = (\n",
    "            train_loss / total_targets if total_targets > 0 else float(\"inf\")\n",
    "        )\n",
    "        avg_dev_loss = (\n",
    "            dev_loss / total_dev_targets if total_dev_targets > 0 else float(\"inf\")\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss}, Dev Loss: {avg_dev_loss}\"\n",
    "        )\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(model, \"trained_np_ckpt.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 301\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[84], line 278\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 278\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass2idx\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m     dev_loss \u001b[38;5;241m=\u001b[39m evaluate_epoch(model, dev_loader, criterion, device, class2idx)\n",
      "Cell \u001b[0;32mIn[84], line 157\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, criterion, optimizer, device, class2idx)\u001b[0m\n\u001b[1;32m    154\u001b[0m active_targets \u001b[38;5;241m=\u001b[39m targets[targets \u001b[38;5;241m!=\u001b[39m class2idx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    155\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, active_targets)\n\u001b[0;32m--> 157\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    160\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/iw/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iw/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/iw/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 1. Define Dataset and DataLoader\n",
    "class CantoneseDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sent_file,\n",
    "        lb_file,\n",
    "        char2idx,\n",
    "        class2idx,\n",
    "        pad_token=\"<PAD>\",\n",
    "        unk_token=\"<UNK>\",\n",
    "    ):\n",
    "        self.sentences, self.labels = self.load_data(sent_file, lb_file)\n",
    "        self.char2idx = char2idx\n",
    "        self.class2idx = class2idx\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.prepared_data = self.prepare_data()\n",
    "\n",
    "    def load_data(self, sent_file, lb_file):\n",
    "        with open(sent_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentences = [line.strip() for line in f]\n",
    "        with open(lb_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            labels = [line.strip().split() for line in f]\n",
    "        return sentences, labels\n",
    "\n",
    "    def prepare_data(self):\n",
    "        input_ids = []\n",
    "        target_ids = []\n",
    "        target_indices = []\n",
    "        for sent, label in zip(self.sentences, self.labels):\n",
    "            input_id = [\n",
    "                self.char2idx.get(char, self.char2idx[self.unk_token]) for char in sent\n",
    "            ]\n",
    "            target_id = [\n",
    "                self.class2idx.get(pron, self.class2idx[self.unk_token])\n",
    "                for pron in label\n",
    "            ]\n",
    "            input_ids.append(input_id)\n",
    "            target_ids.append(target_id)\n",
    "\n",
    "            # Compute target indices for polyphonic characters\n",
    "            target_idx = [i for i, pron in enumerate(label) if pron in self.class2idx]\n",
    "            target_indices.append(target_idx)\n",
    "\n",
    "        # Pad sequences\n",
    "        max_length = max(len(seq) for seq in input_ids)\n",
    "        input_ids = [\n",
    "            seq + [self.char2idx[self.pad_token]] * (max_length - len(seq))\n",
    "            for seq in input_ids\n",
    "        ]\n",
    "        target_ids = [\n",
    "            seq + [self.class2idx[self.pad_token]] * (max_length - len(seq))\n",
    "            for seq in target_ids\n",
    "        ]\n",
    "\n",
    "        return list(zip(input_ids, target_ids, target_indices))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_id, target_id, target_idx = self.prepared_data[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_id, dtype=torch.long),\n",
    "            \"target_ids\": torch.tensor(target_id, dtype=torch.long),\n",
    "            \"target_indices\": target_idx,  # Keep as list for variable lengths\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    targets = torch.stack([item[\"target_ids\"] for item in batch])\n",
    "    target_indices = [item[\"target_indices\"] for item in batch]\n",
    "    return inputs, targets, target_indices\n",
    "\n",
    "\n",
    "# 2. Define the Model\n",
    "class G2pM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, padding_idx):\n",
    "        super(G2pM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Bidirectional\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, inputs, target_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: [batch_size, seq_len]\n",
    "            target_indices: list of lists containing target positions for each sample\n",
    "        Returns:\n",
    "            logits: [total_targets, num_classes]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(inputs)  # [batch_size, seq_len, embed_dim]\n",
    "        packed_output, _ = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim*2]\n",
    "\n",
    "        # Extract target hidden states\n",
    "        target_hidden = []\n",
    "        for i, indices in enumerate(target_indices):\n",
    "            for idx in indices:\n",
    "                if idx < packed_output.size(\n",
    "                    1\n",
    "                ):  # Ensure index is within sequence length\n",
    "                    target_hidden.append(packed_output[i, idx, :])\n",
    "        if target_hidden:\n",
    "            target_hidden = torch.stack(target_hidden)  # [total_targets, hidden_dim*2]\n",
    "        else:\n",
    "            target_hidden = torch.empty(0, self.lstm.hidden_size * 2).to(\n",
    "                packed_output.device\n",
    "            )\n",
    "\n",
    "        logits = self.fc(target_hidden)  # [total_targets, num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 3. Training and Evaluation Functions\n",
    "def train_epoch(model, loader, criterion, optimizer, device, class2idx):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_targets = 0\n",
    "\n",
    "    progress = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for batch in progress:\n",
    "        inputs, targets, target_indices = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs, target_indices)  # [total_targets, num_classes]\n",
    "        if logits.numel() == 0:\n",
    "            continue  # Skip if there are no target indices in the batch\n",
    "\n",
    "        # Flatten targets based on target_indices\n",
    "        active_targets = targets[targets != class2idx[\"<PAD>\"]].view(-1)\n",
    "        loss = criterion(logits, active_targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * logits.size(0)\n",
    "        total_targets += logits.size(0)\n",
    "\n",
    "        avg_loss = running_loss / total_targets if total_targets > 0 else 0.0\n",
    "        progress.set_postfix({\"Loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    avg_loss = running_loss / total_targets if total_targets > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device, class2idx):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_targets = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in progress:\n",
    "            inputs, targets, target_indices = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(inputs, target_indices)  # [total_targets, num_classes]\n",
    "            if logits.numel() == 0:\n",
    "                continue  # Skip if there are no target indices in the batch\n",
    "\n",
    "            active_targets = targets[targets != class2idx[\"<PAD>\"]].view(-1)\n",
    "            loss = criterion(logits, active_targets)\n",
    "\n",
    "            running_loss += loss.item() * logits.size(0)\n",
    "            total_targets += logits.size(0)\n",
    "\n",
    "            avg_loss = running_loss / total_targets if total_targets > 0 else 0.0\n",
    "            progress.set_postfix({\"Loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    avg_loss = running_loss / total_targets if total_targets > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# 4. Evaluation Function for Sentences\n",
    "def evaluate_sentence(\n",
    "    model, sentence, char2idx, idx2class, device, pad_token=\"<PAD>\", unk_token=\"<UNK>\"\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert sentence to indices\n",
    "        input_ids = [char2idx.get(char, char2idx[unk_token]) for char in sentence]\n",
    "        input_tensor = (\n",
    "            torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        )  # [1, seq_len]\n",
    "\n",
    "        # Since it's a single sentence, target_indices are all positions (or specific based on your use case)\n",
    "        # Assuming you want predictions for all characters\n",
    "        target_indices = [list(range(len(input_ids)))]\n",
    "\n",
    "        # Get logits\n",
    "        logits = model(input_tensor, target_indices)  # [seq_len, num_classes]\n",
    "        if logits.numel() == 0:\n",
    "            print(\"No target indices found in the sentence.\")\n",
    "            return []\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()  # [seq_len]\n",
    "\n",
    "        # Map predictions to class labels\n",
    "        predicted_labels = [idx2class.get(idx, \"<UNK>\") for idx in predictions]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "# 5. Main Training Script\n",
    "def main():\n",
    "    # Load mappings\n",
    "    char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "    class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "    # Create inverse mapping for class indices\n",
    "    idx2class = {idx: cls for cls, idx in class2idx.items()}\n",
    "\n",
    "    # Parameters (adjust as needed)\n",
    "    vocab_size = len(char2idx)\n",
    "    embed_dim = 128\n",
    "    hidden_dim = 256\n",
    "    num_classes = len(class2idx)\n",
    "    pad_idx = char2idx[\"<PAD>\"]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CantoneseDataset(\"train.sent\", \"train.lb\", char2idx, class2idx)\n",
    "    dev_dataset = CantoneseDataset(\"dev.sent\", \"dev.lb\", char2idx, class2idx)\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = G2pM(vocab_size, embed_dim, hidden_dim, num_classes, padding_idx=pad_idx)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.001\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8\n",
    "    )\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, class2idx\n",
    "        )\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        dev_loss = evaluate_epoch(model, dev_loader, criterion, device, class2idx)\n",
    "        print(f\"Validation Loss: {dev_loss:.4f}\")\n",
    "\n",
    "        # Optionally, save the model checkpoint\n",
    "        torch.save(model.state_dict(), f\"trained_pytorch_ckpt_epoch{epoch}.pth\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"trained_pytorch_final.pth\")\n",
    "\n",
    "    # Example Evaluation\n",
    "    sentence = \"然而，他红了20年以后，他竟退出了大家的视线。\"\n",
    "    predicted_labels = evaluate_sentence(model, sentence, char2idx, idx2class, device)\n",
    "    print(\"\\nSentence Evaluation:\")\n",
    "    for char, label in zip(sentence, predicted_labels):\n",
    "        print(f\"{char}: {label}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"然而，他红了20年以后，他竟退出了大家的视线。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from g2pM2 import G2pM\n",
    "\n",
    "\n",
    "# Load the trained model weights\n",
    "def load_trained_model(model, ckpt_file):\n",
    "    state_dict = pickle.load(open(ckpt_file, \"rb\"))\n",
    "    model.load_variable(state_dict)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = G2pM()\n",
    "\n",
    "# Load the trained weights\n",
    "load_trained_model(model, \"trained_np_ckpt.pkl\")\n",
    "\n",
    "# Test sentence\n",
    "sentence = \"然而，他红了20年以后，他竟退出了大家的视线。\"\n",
    "\n",
    "# Predict pronunciations\n",
    "predicted_pronunciations = model(sentence, tone=True, char_split=False)\n",
    "\n",
    "# Print the result\n",
    "print(predicted_pronunciations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mToJyutping\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ToJyutping\u001b[38;5;241m.\u001b[39mget_jyutping_text(\u001b[43msentence\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "import ToJyutping\n",
    "\n",
    "ToJyutping.get_jyutping_text(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cong1 baak6 sik1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToJyutping.get_jyutping_text(\"蒼白色\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Load mappings\n",
    "    char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "    class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "    # Create inverse mapping for class indices\n",
    "    idx2class = {idx: cls for cls, idx in class2idx.items()}\n",
    "\n",
    "    # Parameters (adjust as needed)\n",
    "    vocab_size = len(char2idx)\n",
    "    embed_dim = 128\n",
    "    hidden_dim = 256\n",
    "    num_classes = len(class2idx)\n",
    "    pad_idx = char2idx[\"<PAD>\"]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CantoneseDataset(\"train.sent\", \"train.lb\", char2idx, class2idx)\n",
    "    dev_dataset = CantoneseDataset(\"dev.sent\", \"dev.lb\", char2idx, class2idx)\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = G2pM(vocab_size, embed_dim, hidden_dim, num_classes, padding_idx=pad_idx)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Load the trained model\n",
    "    model.load_state_dict(torch.load(\"trained_pytorch_final.pth\", map_location=device))\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Example Evaluation\n",
    "    sentence = \"然而，他红了20年以后，他竟退出了大家的视线。\"\n",
    "    predicted_labels = evaluate_sentence(model, sentence, char2idx, idx2class, device)\n",
    "    print(\"\\nSentence Evaluation:\")\n",
    "    for char, label in zip(sentence, predicted_labels):\n",
    "        print(f\"{char}: {label}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
