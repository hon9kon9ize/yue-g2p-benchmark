{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "BOS_TOKEN = \"시\"\n",
    "EOS_TOKEN = \"끝\"\n",
    "SPLIT_TOKEN = \"▁\"\n",
    "def create_digest_cedict(mono_file, poly_file, output_file):\n",
    "    cedict = {}\n",
    "\n",
    "    # Process monophonic characters\n",
    "    with open(mono_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            char, pron = line.strip().split('\\t')\n",
    "            cedict[char] = [pron]\n",
    "\n",
    "    # Process polyphonic characters\n",
    "    with open(poly_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            char, prons = line.strip().split('\\t')\n",
    "            cedict[char] = prons.split(',')\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cedict, f)\n",
    "\n",
    "# Create digest_cedict.pkl\n",
    "create_digest_cedict('MONOPHONIC_CHARS.txt', 'POLYPHONIC_CHARS.txt', 'digest_cedict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def create_char2idx(sent_files, output_file):\n",
    "    char2idx = {}\n",
    "    idx = 0\n",
    "\n",
    "    for sent_file in sent_files:\n",
    "        with open(sent_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for char in line.strip():\n",
    "                    if char not in char2idx:\n",
    "                        char2idx[char] = idx\n",
    "                        idx += 1\n",
    "\n",
    "    # Add special tokens\n",
    "    char2idx[UNK_TOKEN] = idx\n",
    "    char2idx[PAD_TOKEN] = idx + 1\n",
    "    char2idx[BOS_TOKEN] = idx + 2\n",
    "    char2idx[EOS_TOKEN] = idx + 3\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(char2idx, f)\n",
    "\n",
    "\n",
    "# Create char2idx.pkl\n",
    "create_char2idx([\"train.sent\", \"dev.sent\", \"test.sent\"], \"char2idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class2idx(lb_files, output_file):\n",
    "    class2idx = {}\n",
    "    idx = 0\n",
    "\n",
    "    for lb_file in lb_files:\n",
    "        with open(lb_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for pron in line.strip().split():\n",
    "                    if pron not in class2idx:\n",
    "                        class2idx[pron] = idx\n",
    "                        idx += 1\n",
    "\n",
    "    # Add special tokens\n",
    "    class2idx[UNK_TOKEN] = idx\n",
    "    class2idx[PAD_TOKEN] = idx + 1\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(class2idx, f)\n",
    "\n",
    "\n",
    "# Create class2idx.pkl\n",
    "create_class2idx([\"train.lb\", \"dev.lb\", \"test.lb\"], \"class2idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "def initialize_np_ckpt(char2idx, class2idx, embedding_dim=64, lstm_hidden_dim=32):\n",
    "    state_dict = {}\n",
    "\n",
    "    # Initialize embedding weights\n",
    "    state_dict[\"embedding.weight\"] = np.random.randn(\n",
    "        len(char2idx), embedding_dim\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Initialize LSTM weights and biases\n",
    "    state_dict[\"lstm.weight_ih_l0\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, embedding_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.weight_hh_l0\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.bias_ih_l0\"] = np.zeros(4 * lstm_hidden_dim, dtype=np.float32)\n",
    "    state_dict[\"lstm.bias_hh_l0\"] = np.zeros(4 * lstm_hidden_dim, dtype=np.float32)\n",
    "\n",
    "    state_dict[\"lstm.weight_ih_l0_reverse\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, embedding_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.weight_hh_l0_reverse\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.bias_ih_l0_reverse\"] = np.zeros(\n",
    "        4 * lstm_hidden_dim, dtype=np.float32\n",
    "    )\n",
    "    state_dict[\"lstm.bias_hh_l0_reverse\"] = np.zeros(\n",
    "        4 * lstm_hidden_dim, dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Initialize fully connected layer weights and biases\n",
    "    state_dict[\"logit_layer.0.weight\"] = np.random.randn(\n",
    "        lstm_hidden_dim, 2 * lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"logit_layer.0.bias\"] = np.zeros(lstm_hidden_dim, dtype=np.float32)\n",
    "    state_dict[\"logit_layer.2.weight\"] = np.random.randn(\n",
    "        len(class2idx), lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"logit_layer.2.bias\"] = np.zeros(len(class2idx), dtype=np.float32)\n",
    "\n",
    "    # Save to pickle file\n",
    "    with open(\"np_ckpt.pkl\", \"wb\") as f:\n",
    "        pickle.dump(state_dict, f)\n",
    "\n",
    "\n",
    "# Load char2idx and class2idx\n",
    "char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "\n",
    "# Initialize np_ckpt.pkl\n",
    "initialize_np_ckpt(char2idx, class2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of digest_cedict: 27006\n",
      "Length of char2idx: 6358\n",
      "Length of class2idx: 1593\n",
      "Dimensions of embedding.weight: (6358, 64)\n",
      "Dimensions of lstm.weight_ih_l0: (128, 64)\n",
      "Dimensions of lstm.weight_hh_l0: (128, 32)\n",
      "Dimensions of lstm.bias_ih_l0: (128,)\n",
      "Dimensions of lstm.bias_hh_l0: (128,)\n",
      "Dimensions of lstm.weight_ih_l0_reverse: (128, 64)\n",
      "Dimensions of lstm.weight_hh_l0_reverse: (128, 32)\n",
      "Dimensions of lstm.bias_ih_l0_reverse: (128,)\n",
      "Dimensions of lstm.bias_hh_l0_reverse: (128,)\n",
      "Dimensions of logit_layer.0.weight: (32, 64)\n",
      "Dimensions of logit_layer.0.bias: (32,)\n",
      "Dimensions of logit_layer.2.weight: (1593, 32)\n",
      "Dimensions of logit_layer.2.bias: (1593,)\n"
     ]
    }
   ],
   "source": [
    "digest_cedict = pickle.load(open(\"digest_cedict.pkl\", \"rb\"))\n",
    "char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "\n",
    "# Print statistics\n",
    "print(\"Length of digest_cedict:\", len(digest_cedict))\n",
    "print(\"Length of char2idx:\", len(char2idx))\n",
    "print(\"Length of class2idx:\", len(class2idx))\n",
    "state_dict = pickle.load(open(\"np_ckpt.pkl\", \"rb\"))\n",
    "\n",
    "print(\"Dimensions of embedding.weight:\", state_dict[\"embedding.weight\"].shape)\n",
    "print(\"Dimensions of lstm.weight_ih_l0:\", state_dict[\"lstm.weight_ih_l0\"].shape)\n",
    "print(\"Dimensions of lstm.weight_hh_l0:\", state_dict[\"lstm.weight_hh_l0\"].shape)\n",
    "print(\"Dimensions of lstm.bias_ih_l0:\", state_dict[\"lstm.bias_ih_l0\"].shape)\n",
    "print(\"Dimensions of lstm.bias_hh_l0:\", state_dict[\"lstm.bias_hh_l0\"].shape)\n",
    "print(\n",
    "    \"Dimensions of lstm.weight_ih_l0_reverse:\",\n",
    "    state_dict[\"lstm.weight_ih_l0_reverse\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"Dimensions of lstm.weight_hh_l0_reverse:\",\n",
    "    state_dict[\"lstm.weight_hh_l0_reverse\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"Dimensions of lstm.bias_ih_l0_reverse:\",\n",
    "    state_dict[\"lstm.bias_ih_l0_reverse\"].shape,\n",
    ")\n",
    "print(\n",
    "    \"Dimensions of lstm.bias_hh_l0_reverse:\",\n",
    "    state_dict[\"lstm.bias_hh_l0_reverse\"].shape,\n",
    ")\n",
    "print(\"Dimensions of logit_layer.0.weight:\", state_dict[\"logit_layer.0.weight\"].shape)\n",
    "print(\"Dimensions of logit_layer.0.bias:\", state_dict[\"logit_layer.0.bias\"].shape)\n",
    "print(\"Dimensions of logit_layer.2.weight:\", state_dict[\"logit_layer.2.weight\"].shape)\n",
    "print(\"Dimensions of logit_layer.2.bias:\", state_dict[\"logit_layer.2.bias\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.4511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3043\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.9429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8303\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.2080\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0434\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9268\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9741\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8937\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9187\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9245\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.9141\n",
      "\n",
      "Sentence Evaluation:\n",
      "然: fung1\n",
      "而: fung1\n",
      "，: faa3\n",
      "他: fung1\n",
      "红: fung1\n",
      "了: fung1\n",
      "2: fung1\n",
      "0: fung1\n",
      "年: fung1\n",
      "以: fung1\n",
      "后: fung1\n",
      "，: fung1\n",
      "他: fung1\n",
      "竟: fung1\n",
      "退: laa1\n",
      "出: laa1\n",
      "了: laa1\n",
      "大: leoi4\n",
      "家: laa1\n",
      "的: laa1\n",
      "视: laa1\n",
      "线: ngaat6\n",
      "。: cak1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 1. Define Dataset and DataLoader\n",
    "class CantoneseDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sent_file,\n",
    "        lb_file,\n",
    "        char2idx,\n",
    "        class2idx,\n",
    "        pad_token=\"<PAD>\",\n",
    "        unk_token=\"<UNK>\",\n",
    "    ):\n",
    "        self.sentences, self.labels = self.load_data(sent_file, lb_file)\n",
    "        self.char2idx = char2idx\n",
    "        self.class2idx = class2idx\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.prepared_data = self.prepare_data()\n",
    "\n",
    "    def load_data(self, sent_file, lb_file):\n",
    "        with open(sent_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentences = [line.strip() for line in f]\n",
    "        with open(lb_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            labels = [line.strip().split() for line in f]\n",
    "        return sentences, labels\n",
    "\n",
    "    def prepare_data(self):\n",
    "        input_ids = []\n",
    "        target_ids = []\n",
    "        target_indices = []\n",
    "        for sent, label in zip(self.sentences, self.labels):\n",
    "            input_id = [\n",
    "                self.char2idx.get(char, self.char2idx[self.unk_token]) for char in sent\n",
    "            ]\n",
    "            target_id = [\n",
    "                self.class2idx.get(pron, self.class2idx[self.unk_token])\n",
    "                for pron in label\n",
    "            ]\n",
    "            input_ids.append(input_id)\n",
    "            target_ids.append(target_id)\n",
    "\n",
    "            # Compute target indices for polyphonic characters\n",
    "            target_idx = [i for i, pron in enumerate(label) if pron in self.class2idx]\n",
    "            target_indices.append(target_idx)\n",
    "\n",
    "        # Pad sequences\n",
    "        max_length = max(len(seq) for seq in input_ids)\n",
    "        input_ids = [\n",
    "            seq + [self.char2idx[self.pad_token]] * (max_length - len(seq))\n",
    "            for seq in input_ids\n",
    "        ]\n",
    "        target_ids = [\n",
    "            seq + [self.class2idx[self.pad_token]] * (max_length - len(seq))\n",
    "            for seq in target_ids\n",
    "        ]\n",
    "\n",
    "        return list(zip(input_ids, target_ids, target_indices))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prepared_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_id, target_id, target_idx = self.prepared_data[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_id, dtype=torch.long),\n",
    "            \"target_ids\": torch.tensor(target_id, dtype=torch.long),\n",
    "            \"target_indices\": target_idx,  # Keep as list for variable lengths\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    targets = torch.stack([item[\"target_ids\"] for item in batch])\n",
    "    target_indices = [item[\"target_indices\"] for item in batch]\n",
    "    return inputs, targets, target_indices\n",
    "\n",
    "\n",
    "# 2. Define the Model\n",
    "class G2pM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, padding_idx):\n",
    "        super(G2pM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # Bidirectional\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, inputs, target_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: [batch_size, seq_len]\n",
    "            target_indices: list of lists containing target positions for each sample\n",
    "        Returns:\n",
    "            logits: [total_targets, num_classes]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(inputs)  # [batch_size, seq_len, embed_dim]\n",
    "        packed_output, _ = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim*2]\n",
    "\n",
    "        # Extract target hidden states\n",
    "        target_hidden = []\n",
    "        for i, indices in enumerate(target_indices):\n",
    "            for idx in indices:\n",
    "                if idx < packed_output.size(\n",
    "                    1\n",
    "                ):  # Ensure index is within sequence length\n",
    "                    target_hidden.append(packed_output[i, idx, :])\n",
    "        if target_hidden:\n",
    "            target_hidden = torch.stack(target_hidden)  # [total_targets, hidden_dim*2]\n",
    "        else:\n",
    "            target_hidden = torch.empty(0, self.lstm.hidden_size * 2).to(\n",
    "                packed_output.device\n",
    "            )\n",
    "\n",
    "        logits = self.fc(target_hidden)  # [total_targets, num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 3. Training and Evaluation Functions\n",
    "def train_epoch(model, loader, criterion, optimizer, device, class2idx):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_targets = 0\n",
    "\n",
    "    progress = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for batch in progress:\n",
    "        inputs, targets, target_indices = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs, target_indices)  # [total_targets, num_classes]\n",
    "        if logits.numel() == 0:\n",
    "            continue  # Skip if there are no target indices in the batch\n",
    "\n",
    "        # Flatten targets based on target_indices\n",
    "        active_targets = targets[targets != class2idx[\"<PAD>\"]].view(-1)\n",
    "        loss = criterion(logits, active_targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * logits.size(0)\n",
    "        total_targets += logits.size(0)\n",
    "\n",
    "        avg_loss = running_loss / total_targets if total_targets > 0 else 0.0\n",
    "        progress.set_postfix({\"Loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    avg_loss = running_loss / total_targets if total_targets > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate_epoch(model, loader, criterion, device, class2idx):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_targets = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in progress:\n",
    "            inputs, targets, target_indices = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(inputs, target_indices)  # [total_targets, num_classes]\n",
    "            if logits.numel() == 0:\n",
    "                continue  # Skip if there are no target indices in the batch\n",
    "\n",
    "            active_targets = targets[targets != class2idx[\"<PAD>\"]].view(-1)\n",
    "            loss = criterion(logits, active_targets)\n",
    "\n",
    "            running_loss += loss.item() * logits.size(0)\n",
    "            total_targets += logits.size(0)\n",
    "\n",
    "            avg_loss = running_loss / total_targets if total_targets > 0 else 0.0\n",
    "            progress.set_postfix({\"Loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    avg_loss = running_loss / total_targets if total_targets > 0 else 0.0\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# 4. Evaluation Function for Sentences\n",
    "def evaluate_sentence(\n",
    "    model, sentence, char2idx, idx2class, device, pad_token=\"<PAD>\", unk_token=\"<UNK>\"\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert sentence to indices\n",
    "        input_ids = [char2idx.get(char, char2idx[unk_token]) for char in sentence]\n",
    "        input_tensor = (\n",
    "            torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        )  # [1, seq_len]\n",
    "\n",
    "        # Since it's a single sentence, target_indices are all positions (or specific based on your use case)\n",
    "        # Assuming you want predictions for all characters\n",
    "        target_indices = [list(range(len(input_ids)))]\n",
    "\n",
    "        # Get logits\n",
    "        logits = model(input_tensor, target_indices)  # [seq_len, num_classes]\n",
    "        if logits.numel() == 0:\n",
    "            print(\"No target indices found in the sentence.\")\n",
    "            return []\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()  # [seq_len]\n",
    "\n",
    "        # Map predictions to class labels\n",
    "        predicted_labels = [idx2class.get(idx, \"<UNK>\") for idx in predictions]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "# 5. Main Training Script\n",
    "def main():\n",
    "    # Load mappings\n",
    "    char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "    class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "    # Create inverse mapping for class indices\n",
    "    idx2class = {idx: cls for cls, idx in class2idx.items()}\n",
    "\n",
    "    # Parameters (adjust as needed)\n",
    "    vocab_size = len(char2idx)\n",
    "    embed_dim = 128\n",
    "    hidden_dim = 256\n",
    "    num_classes = len(class2idx)\n",
    "    pad_idx = char2idx[\"<PAD>\"]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CantoneseDataset(\"train.sent\", \"train.lb\", char2idx, class2idx)\n",
    "    dev_dataset = CantoneseDataset(\"dev.sent\", \"dev.lb\", char2idx, class2idx)\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = G2pM(vocab_size, embed_dim, hidden_dim, num_classes, padding_idx=pad_idx)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.001\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8\n",
    "    )\n",
    "\n",
    "    # Training parameters\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, class2idx\n",
    "        )\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        dev_loss = evaluate_epoch(model, dev_loader, criterion, device, class2idx)\n",
    "        print(f\"Validation Loss: {dev_loss:.4f}\")\n",
    "\n",
    "        # Optionally, save the model checkpoint\n",
    "        torch.save(model.state_dict(), f\"trained_pytorch_ckpt_epoch{epoch}.pth\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"trained_pytorch_final.pth\")\n",
    "\n",
    "    # Example Evaluation\n",
    "    sentence = \"然而，他红了20年以后，他竟退出了大家的视线。\"\n",
    "    predicted_labels = evaluate_sentence(model, sentence, char2idx, idx2class, device)\n",
    "    print(\"\\nSentence Evaluation:\")\n",
    "    for char, label in zip(sentence, predicted_labels):\n",
    "        print(f\"{char}: {label}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"然而，他红了20年以后，他竟退出了大家的视线。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jin4 ji4, taa1 hung4 liu5 […] nin4 ji5 hau6, taa1 ging2 teoi3 ceot1 liu5 daai6 gaa1 dik1 si6 sin3.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ToJyutping\n",
    "\n",
    "ToJyutping.get_jyutping_text(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fung1', 'fung1', 'faa3', 'fung1', 'fung1', 'fung1', 'fung1', 'fung1', 'fung1', 'fung1', 'fung1', 'fung1', 'fung1', 'fung1', 'laa1', 'laa1', 'laa1', 'leoi4', 'laa1', 'laa1', 'laa1', 'ngaat6', 'cak1']\n",
      "\n",
      "Sentence Evaluation:\n",
      "然: fung1\n",
      "而: fung1\n",
      "，: faa3\n",
      "他: fung1\n",
      "红: fung1\n",
      "了: fung1\n",
      "2: fung1\n",
      "0: fung1\n",
      "年: fung1\n",
      "以: fung1\n",
      "后: fung1\n",
      "，: fung1\n",
      "他: fung1\n",
      "竟: fung1\n",
      "退: laa1\n",
      "出: laa1\n",
      "了: laa1\n",
      "大: leoi4\n",
      "家: laa1\n",
      "的: laa1\n",
      "视: laa1\n",
      "线: ngaat6\n",
      "。: cak1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8d/b6tnc2p90l548qg78nc_k0_m0000gn/T/ipykernel_81604/3975840178.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"trained_pytorch_final.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # Load mappings\n",
    "    char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "    class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "    # Create inverse mapping for class indices\n",
    "    idx2class = {idx: cls for cls, idx in class2idx.items()}\n",
    "\n",
    "    # Parameters (adjust as needed)\n",
    "    vocab_size = len(char2idx)\n",
    "    embed_dim = 128\n",
    "    hidden_dim = 256\n",
    "    num_classes = len(class2idx)\n",
    "    pad_idx = char2idx[\"<PAD>\"]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CantoneseDataset(\"train.sent\", \"train.lb\", char2idx, class2idx)\n",
    "    dev_dataset = CantoneseDataset(\"dev.sent\", \"dev.lb\", char2idx, class2idx)\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = G2pM(vocab_size, embed_dim, hidden_dim, num_classes, padding_idx=pad_idx)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Load the trained model\n",
    "    model.load_state_dict(torch.load(\"trained_pytorch_final.pth\", map_location=device))\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Example Evaluation\n",
    "    sentence = \"然而，他红了20年以后，他竟退出了大家的视线。\"\n",
    "    predicted_labels = evaluate_sentence(model, sentence, char2idx, idx2class, device)\n",
    "    print(predicted_labels)\n",
    "    print(\"\\nSentence Evaluation:\")\n",
    "    for char, label in zip(sentence, predicted_labels):\n",
    "        print(f\"{char}: {label}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
